\section{Related Work}
Pang, Lee, and Vaithyanathan implemented sentiment analysis using three such supervised learning methods \cite{Pang:2002:TUS:1118693.1118704}. Focusing on movie reviews because they are both readily available and difficult, the authors used the Naive Bayes, Maximum Entropy, and Support Vector Machines machine learning approaches to attempt to categorize the reviews, after a precursory exercise in essentially counting the prevalence of predefined positive and negative words in the documents. While the chosen words were pretty poor in reflection of common internet posts, the results were relatively impressive at 69\% accuracy. Naive Bayes is known as the least complex and most simplistic of the three tested procedures, and relies on an assumption that the document’s features or sentiment-associated words are conditionally independent, which is clearly false when considering documents with an overarching theme or sentiment. While the settings for the tools used to complete two methods were supposedly standard, the authors made no mention as to how the settings, or the specific algorithms, were selected, leaving possibility for skewed results. The comparative results were mostly to be expected, as the most complex SVM method achieved the best results of the three in the majority of tests of varying feature-defining complexity, hovering at roughly an 82\% accuracy rating. While beating the simplistic precursory tests, performances were noticeably worse when compared to traditional category filtering, thus specialized methods must be used to categorize based on sentiment.

Dave, Lawrence, and Pennock attempted to put into adjust traditional algorithms with weighing and scoring functions, however the results were a little erratic \cite{Dave:2003:MPG:775152.775226}. They attempted to replicate the methodology by Peng et al. by using the same SVM package and got similar good performance with one test, though worse performance than the precursory baseline methods in the second. Each primary methodology used resulted in relatively equivalent and poor results with the second test, while some alternative methods seemed to only be accurate for one of the two tests, which brings into question the quality and uniformity of the data itself. As a result, the authors chose to essentially discard the second test when evaluating and creating their specialized method, which is essentially the aforementioned simplistic Naive Bayes with various smoothing methods bruteforced in attempt to achieve better results. Essentially, the results show nothing other than that their specialized method of smoothing and reweighing features was moderately more successful in this one particular test, only maximizing at 5\% higher than the standard methods. However, this did not stop them from using the method to perform data mining, and then draw conclusions based on a set of data returned from their own inaccurate methodology. If nothing else, this shows the difficulty of accurate sentiment analysis with more traditional supervised machine learning techniques.

Though supervised methods are the primary ones being focused on for this problem, Peter Turney attempted the sentiment analysis using unsupervised methods \cite{Turney:2002:TUT:1073083.1073153}. As adjectives and adverbs are often used to express positive or negative sentiment, the method first extracts such parts of speech with an accompanying word to provide context, as it is otherwise unknown as to what the sentiment is being directed toward. Next, the orientation of the phrases is measured using a pointwise mutual information formula, taking the ratio between the co-occurrence probability of two terms and the probability that the co-occur if they were statistically independent, and taking the logarithm of the result. This formula is used to compare phrases to known keywords in order to obtain the Semantic Orientation of the phrase. Applying this to the web, the PMI of a phrase is simply adjusted through “some minor algebraic manipulation” to be the number of hits of a particular word within a certain distance from the keywords, when passed through an internet search engine. The keywords used in this paper are “excellent” and “poor”, frankly two poor choices when taking results straight off the internet via a search engine. Furthermore, the only search engine used is AltaVista purely because of its ability to take distance between phrases into account. Domains also must be taken into account, as a word with a positive association within a given scope may have a negative association with other types of products or reviewers. The results were quite varying, with a slightly higher accuracy rating of 84\% in the domain of automobiles, though much lower accuracy when considering the original domain of movie reviews, at a 65.83\%. While unimpressive, the questionable keyword choice and reliance on a single search engine are possible contributing factors.
